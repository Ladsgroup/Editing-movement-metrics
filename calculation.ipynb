{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import re\n",
    "import io\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import wmfdata as wmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV file where metrics are or will be saved\n",
    "FILENAME = \"metrics.tsv\"\n",
    "\n",
    "# Latest mediawiki_history snapshot in Hive\n",
    "SNAPSHOT = \"2018-04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    old_metrics = pd.read_csv(FILENAME, sep=\"\\t\", parse_dates = [\"month\"])\n",
    "    START = old_metrics[\"month\"].max() + relativedelta(months=1)\n",
    "except FileNotFoundError:\n",
    "    START = pd.Timestamp(2001, 1, 1)\n",
    "    old_metrics = None\n",
    "\n",
    "START = START.strftime(\"%Y-%m-%d\")\n",
    "print(START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-query metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb_queries = {\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.sql\",\n",
    "    },\n",
    "    \"nonbot_edits\": {\n",
    "        \"file\": \"queries/nonbot_edits.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "hive_queries = {\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.hql\",\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.hql\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in mdb_queries:\n",
    "    q = mdb_queries[k]\n",
    "    with open(q[\"file\"]) as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    wmf.utils.print_err(\"Running {}...\".format(k))\n",
    "    q[\"result\"] = wmf.mariadb.run(text.format(start = START))\n",
    "    q[\"result\"][\"month\"] = pd.to_datetime(q[\"result\"][\"month\"])\n",
    "    \n",
    "for k in hive_queries:\n",
    "    q = hive_queries[k]\n",
    "    with open(q[\"file\"]) as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    wmf.utils.print_err(\"Running {}...\".format(k))\n",
    "    q[\"result\"] = wmf.hive.run(text.format(start = START, snapshot = SNAPSHOT))\n",
    "    # Unlike our MariaDB queries, the Hive query returns a string rather than a date\n",
    "    q[\"result\"][\"month\"] = pd.to_datetime(q[\"result\"][\"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiquery metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir data\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: remove old files matching this pattern\n",
    "#!cp /mnt/data/xmldatadumps/public/other/wikistats_1/*_main.zip data\n",
    "!cp /mnt/data/xmldatadumps/public/other/pagecounts-ez/wikistats/csv_*_main.zip data\n",
    "\n",
    "zipfiles = !ls data/csv_*_main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"wiki\", \"group\", \"month\", \"articles\", \"files\"]\n",
    "content = pd.DataFrame(columns=cols)\n",
    "\n",
    "for f in zipfiles:\n",
    "    # Extract the Wikistats code for the project family\n",
    "    grp = re.search(r\"data/csv_([a-z]{2})_main.zip\", f).group(1)\n",
    "    \n",
    "    # Map Wikistats codes for project family to the corresponding database codes\n",
    "    db_suffix = {\n",
    "        \"wb\": \"wikibooks\",\n",
    "        \"wk\": \"wiktionary\",\n",
    "        \"wn\": \"wikinews\",\n",
    "        \"wo\": \"wikivoyage\",\n",
    "        \"wp\": \"wiki\",\n",
    "        \"wq\": \"wikiquote\",\n",
    "        \"ws\": \"wikisource\",\n",
    "        \"wv\": \"wikiversity\",\n",
    "        \"wx\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Unzip files to stdout and capture it in an IPython SList.\n",
    "    # Put the newline-separated string (`.n`) of the output in a buffer for Pandas.\n",
    "    sm = !unzip -p {f} StatisticsMonthly.csv\n",
    "    sm = io.StringIO(sm.n)\n",
    "    \n",
    "    spn = !unzip -p {f} StatisticsPerNamespace.csv\n",
    "    spn = io.StringIO(spn.n)\n",
    "     \n",
    "    # Manually set column numbers because some CSVs are ragged \n",
    "    # Select the columns we need, which aren't named so we need to select by location\n",
    "    art = pd.read_csv(sm, header=None, usecols=[0, 1, 6], names=range(29))\n",
    "    art.columns = [\"wiki\", \"month\", \"articles\"]\n",
    "\n",
    "    # Wikisource has extra namespaces so its file has more columns\n",
    "    if grp == \"ws\":\n",
    "        col_nums = range(22)\n",
    "    else:\n",
    "        col_nums = range(17)\n",
    "    \n",
    "    files = pd.read_csv(spn, header=None, usecols=[0, 1, 5], names=col_nums)\n",
    "    files.columns = [\"wiki\", \"month\", \"files\"]\n",
    "        \n",
    "    grp_content = pd.merge(art, files, on=[\"wiki\", \"month\"], validate=\"one_to_one\")\n",
    "        \n",
    "    # Wiki column just contains the language code (except in wx) so we have to disambiguate across files\n",
    "    grp_content[\"wiki\"] = grp_content[\"wiki\"] + db_suffix[grp]\n",
    "    \n",
    "    grp_content[\"group\"] = grp\n",
    "    \n",
    "    content = content.append(grp_content)\n",
    "    \n",
    "    sm.close()\n",
    "    spn.close()\n",
    "\n",
    "content[\"month\"] = pd.to_datetime(content[\"month\"])\n",
    "content[\"articles\"] = content[\"articles\"].astype(int)\n",
    "\n",
    "# Remove \"wikis\" with zz codes since those are aggregates\n",
    "not_zz = lambda df: ~df[\"wiki\"].str.match(r\"zz.*\")\n",
    "content = content[not_zz]\n",
    "\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content is articles + files on all wikis except Commons, \n",
    "# where it's files alone since there files count as articles\n",
    "def count_content(df):\n",
    "    files = df[\"files\"].sum()\n",
    "    noncommons_articles = df[df[\"wiki\"] != \"commons\"][\"articles\"].sum()\n",
    "    total_content = noncommons_articles + files\n",
    "    wikipedia_articles = df[df[\"group\"] == \"wp\"][\"articles\"].sum()\n",
    "    wikidata_entities = df[df[\"wiki\"] == \"wikidata\"][\"articles\"].sum()\n",
    "\n",
    "    return pd.Series(\n",
    "        [total_content, wikipedia_articles, files, wikidata_entities],\n",
    "        index=[\"total_content\", \"wikipedia_articles\", \"files\", \"wikidata_entities\"]\n",
    "    )\n",
    "\n",
    "glob_cont = content.groupby(\"month\").apply(count_content)\n",
    "glob_cont.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dates with 0 articles, because those are junk data\n",
    "glob_cont = glob_cont[glob_cont[\"wikipedia_articles\"] != 0]\n",
    "\n",
    "# This data is calculated as of the end of a calendar month. In other places,\n",
    "# the metric is dated the first day of that month it applies to. Let's convert\n",
    "# to that.\n",
    "glob_cont.index = glob_cont.index - pd.tseries.offsets.MonthBegin()\n",
    "\n",
    "glob_cont.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and saving metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [mdb_queries[k][\"result\"] for k in mdb_queries]\n",
    "dfs.extend([hive_queries[k][\"result\"] for k in hive_queries])\n",
    "dfs.append(glob_cont)\n",
    "new_metrics = reduce(lambda l, r: pd.merge(l, r), dfs)\n",
    "\n",
    "if old_metrics:\n",
    "    metrics = pd.concat([old_metrics, new_metrics])\n",
    "else:\n",
    "    metrics = new_metrics\n",
    "    \n",
    "metrics.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in metrics.columns:\n",
    "    if col != \"month\":\n",
    "        metrics[col] = metrics[col].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv(FILENAME, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
