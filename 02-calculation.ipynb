{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV file where metrics are or will be saved\n",
    "FILENAME = \"metrics/metrics.tsv\"\n",
    "\n",
    "# Metric month. The mediawiki_history snapshot for this month must be available.\n",
    "METRICS_MONTH_TEXT = \"2018-09\"\n",
    "MEDIAWIKI_HISTORY_SNAPSHOT = \"2018-10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You can find the source for `wmfdata` at https://github.com/neilpquinn/wmfdata\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from functools import reduce\n",
    "import io\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "import wmfdata as wmf\n",
    "from wmfdata import hive, mariadb\n",
    "from wmfdata.utils import mediawiki_dt, print_err, pd_display_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our metrics month to all the formats we need and provide them in a dict\n",
    "# so we can easily use them to format strings\n",
    "metrics_month = pd.Period(METRICS_MONTH_TEXT)\n",
    "date_params = {\n",
    "    \"mediawiki_history_snapshot\": MEDIAWIKI_HISTORY_SNAPSHOT,\n",
    "    \"metrics_month\": str(metrics_month),\n",
    "    \"metrics_month_start\": str(metrics_month.start_time), \n",
    "    \"metrics_month_first_day\": str(metrics_month.asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_month_end\": str((metrics_month + 1).start_time),\n",
    "    \"metrics_month_last_day\": str(metrics_month.asfreq(\"D\", how=\"end\")),\n",
    "    \"api_metrics_month_first_day\": metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"api_metrics_month_day_after\": (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"retention_cohort\": str(metrics_month - 2)\n",
    "}\n",
    "\n",
    "# Load any previous results\n",
    "try:\n",
    "    old_metrics = (\n",
    "        pd.read_csv(FILENAME, sep=\"\\t\", parse_dates = [\"month\"])\n",
    "        .set_index(\"month\")\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    old_metrics = None\n",
    "    \n",
    "def prepare_query(filename):\n",
    "    return (\n",
    "        Path(filename)\n",
    "        .read_text()\n",
    "        .format(**date_params)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MariaDB and Hive query metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_new_editor_retention on hive...\n",
      "Running new_editor_retention on hive...\n",
      "Running global_south_edits_editors on hive...\n"
     ]
    }
   ],
   "source": [
    "queries = {\n",
    "    # To-do: active editors with null registration aren't classified as existing (?)\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\",\n",
    "        \"engine\": \"mariadb\"\n",
    "    },\n",
    "    \"mobile_edits\": {\n",
    "        \"file\": \"queries/mobile_edits.sql\",\n",
    "        \"engine\": \"mariadb\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.hql\",\n",
    "         \"engine\": \"hive\"\n",
    "    },\n",
    "    \"global_south_edits_editors\": {\n",
    "        \"file\": \"queries/global_south_edits_editors.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"mobile-heavy_edits_editors\": {\n",
    "        \"file\": \"queries/mobile-heavy_edits_editors.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"mobile-heavy_new_editor_retention\": {\n",
    "        \"file\": \"queries/mobile-heavy_new_editor_retention.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for key, val in queries.items():\n",
    "    query = prepare_query(val[\"file\"])\n",
    "    engine = val[\"engine\"]\n",
    "    print_err(\"Running {} on {}...\".format(key, engine))\n",
    "    \n",
    "    if engine == \"mariadb\":\n",
    "        result = mariadb.run(query)\n",
    "    elif engine == \"hive\":\n",
    "        result = hive.run(query)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown engine specified.\") \n",
    "    \n",
    "    result = result.assign(month=lambda df: pd.to_datetime(df[\"month\"]))\n",
    "    val[\"result\"] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    \"https://wikimedia.org/api/rest_v1/metrics/edited-pages/new/\" +\n",
    "    \"{project}/all-editor-types/{page_type}/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"https://github.com/wikimedia-research/Editing-movement-metrics (bot)\"\n",
    "}\n",
    "\n",
    "# Create container for results\n",
    "api_results = []\n",
    "\n",
    "def get_new_pages(\n",
    "    project=\"all-projects\",\n",
    "    page_type=\"content\",\n",
    "    start=date_params[\"api_metrics_month_first_day\"],\n",
    "    end=date_params[\"api_metrics_month_day_after\"]\n",
    "):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project = project,\n",
    "        page_type = page_type,\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "    data = r.json()[\"items\"][0][\"results\"]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"])\n",
    "    frame = frame.rename(columns={\"timestamp\": \"month\"})\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new = get_new_pages().rename(columns={\"new_pages\": \"net_new_content_pages\"})\n",
    "api_results.append(total_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wd = get_new_pages(\n",
    "    project=\"wikidata.org\"\n",
    ").rename(columns={\n",
    "    \"new_pages\": \"net_new_Wikidata_entities\"\n",
    "})\n",
    "api_results.append(new_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_commons = get_new_pages(\n",
    "    project=\"commons.wikimedia.org\"\n",
    ").rename(columns={\n",
    "    \"new_pages\": \"net_new_Commons_content_pages\"\n",
    "})\n",
    "api_results.append(new_commons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of project URLs (each one in a 1-tuple)\n",
    "wp_domains = wmf.mariadb.run(\"\"\"\n",
    "select trim(leading \".\" from reverse(site_domain))\n",
    "from enwiki.sites\n",
    "where site_group = \"wikipedia\"\n",
    "\"\"\", fmt=\"raw\")\n",
    "\n",
    "# Query the API for each project and append records to a list\n",
    "results = []\n",
    "n = len(wp_domains)\n",
    "\n",
    "for idx, val in enumerate(wp_domains):\n",
    "    domain = val[0]\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        msg = \"Now on the {}th project of {} ({})\"\n",
    "        print_err(msg.format(idx, n, domain))\n",
    "        \n",
    "    frame = get_new_pages(project=domain).reset_index()\n",
    "    frame[\"project\"] = domain\n",
    "    records = frame.to_dict(\"records\")\n",
    "    results.extend(records)\n",
    "    \n",
    "    # Sleep 20 milliseconds\n",
    "    time.sleep(0.02)\n",
    "\n",
    "# Turn the big list of records into a data frame\n",
    "new_per_wp = pd.DataFrame(results)\n",
    "\n",
    "# Sum across projects to get new Wikipedia articles per month\n",
    "new_wp = new_per_wp.groupby(\"month\").agg(\n",
    "    {\"new_pages\": \"sum\"}\n",
    ").rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"}).reset_index()\n",
    "\n",
    "api_results.append(new_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and saving metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble list of result dataframes\n",
    "results = [val[\"result\"] for _, val in queries.items()]\n",
    "results.extend(api_results)\n",
    "\n",
    "# Merge them all, assuming that the month is the only common column\n",
    "new_metrics = reduce(lambda l, r: pd.merge(l, r, how=\"outer\"), results)\n",
    "\n",
    "# Set the month as an index so combine_first works properly\n",
    "new_metrics = new_metrics.set_index(\"month\").sort_index()\n",
    "\n",
    "if old_metrics is None:\n",
    "    metrics = new_metrics\n",
    "else:\n",
    "    metrics = new_metrics.combine_first(old_metrics)\n",
    "    \n",
    "pd_display_all(metrics.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv(FILENAME, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
