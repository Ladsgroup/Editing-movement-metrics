{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import io\n",
    "from functools import reduce\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata.utils import print_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV file where metrics are or will be saved\n",
    "FILENAME = \"metrics/metrics.tsv\"\n",
    "\n",
    "# Metric month\n",
    "METRICS_MONTH = \"2018-09\"\n",
    "\n",
    "# Latest mediawiki_history snapshot in Hive\n",
    "SNAPSHOT = \"2018-09\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = pd.Period(METRICS_MONTH)\n",
    "mm_first_day = mm_period.asfreq(\"D\", how=\"start\")\n",
    "mm_first_day_str = mm_first_day.strftime(\"%Y-%m-%d\")\n",
    "mm_last_day = mm_period.asfreq(\"D\", how=\"end\")\n",
    "mm_last_day_str = mm_last_day.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-01\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    old_metrics = pd.read_csv(FILENAME, sep=\"\\t\", parse_dates = [\"month\"])\n",
    "    old_metrics = old_metrics.set_index(\"month\")\n",
    "    first_null = old_metrics[old_metrics.isnull().any(axis=1)][\"month\"].min()\n",
    "    START = first_null + relativedelta(months=1)\n",
    "except FileNotFoundError:\n",
    "    START = pd.Timestamp(2001, 1, 1)\n",
    "    old_metrics = None\n",
    "\n",
    "START = START.strftime(\"%Y-%m-%d\")\n",
    "print(START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-query metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb_queries = {\n",
    "    \n",
    "    # To-do: active editors with null registration aren't classified as existing (?)\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/mobile_edits.sql\",\n",
    "    }\n",
    "}\n",
    "\n",
    "hive_queries = {\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.hql\",\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.hql\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running active_editors...\n",
      "Running edits...\n"
     ]
    }
   ],
   "source": [
    "for k in mdb_queries:\n",
    "    q = mdb_queries[k]\n",
    "    with open(q[\"file\"]) as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    wmf.utils.print_err(\"Running {}...\".format(k))\n",
    "    q[\"result\"] = wmf.mariadb.run(text.format(start = START))\n",
    "    q[\"result\"][\"month\"] = pd.to_datetime(q[\"result\"][\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running new_editor_retention...\n",
      "Running edits...\n"
     ]
    }
   ],
   "source": [
    "for k in hive_queries:\n",
    "    q = hive_queries[k]\n",
    "    with open(q[\"file\"]) as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    wmf.utils.print_err(\"Running {}...\".format(k))\n",
    "    q[\"result\"] = wmf.hive.run(text.format(start = START, snapshot = SNAPSHOT))\n",
    "    # Unlike our MariaDB queries, the Hive query returns a string rather than a date\n",
    "    q[\"result\"][\"month\"] = pd.to_datetime(q[\"result\"][\"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20181001'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_end_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    \"https://wikimedia.org/api/rest_v1/metrics/edited-pages/new/\" +\n",
    "    \"{project}/all-editor-types/{page_type}/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"https://github.com/wikimedia-research/Editing-movement-metrics (bot)\"\n",
    "}\n",
    "\n",
    "api_end_string = (mm + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\")\n",
    "\n",
    "def get_new_pages(project=\"all-projects\", page_type=\"content\", start=\"20010101\", end=api_end_string):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project = project,\n",
    "        page_type = page_type,\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "    data = r.json()[\"items\"][0][\"results\"]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"])\n",
    "    frame = frame.rename(columns={\"timestamp\": \"month\"})\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new = get_new_pages().rename(columns={\"new_pages\": \"net_new_content_pages\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wd = get_new_pages(\n",
    "    project=\"wikidata.org\",\n",
    "    end=\"20181002\"\n",
    ").rename(columns={\n",
    "    \"new_pages\": \"net_new_Wikidata_entities\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_commons = get_new_pages(\n",
    "    project=\"commons.wikimedia.org\",\n",
    "    end=\"20181002\"\n",
    ").rename(columns={\n",
    "    \"new_pages\": \"net_new_Commons_content_pages\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of project URLs (each one in a 1-tuple)\n",
    "wp_domains = wmf.mariadb.run(\"\"\"\n",
    "select trim(leading \".\" from reverse(site_domain))\n",
    "from enwiki.sites\n",
    "where site_group = \"wikipedia\"\n",
    "\"\"\", fmt=\"raw\")\n",
    "\n",
    "# Query the API for each project and append records to a list\n",
    "results = []\n",
    "n = len(wp_domains)\n",
    "\n",
    "for idx, val in enumerate(wp_domains):\n",
    "    domain = val[0]\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        msg = \"Now on the {}th project of {} ({})\"\n",
    "        print_err(msg.format(idx, n, domain))\n",
    "        \n",
    "    frame = get_new_pages(project=domain).reset_index()\n",
    "    frame[\"project\"] = domain\n",
    "    records = frame.to_dict(\"records\")\n",
    "    results.extend(records)\n",
    "    \n",
    "    # Sleep 50 milliseconds\n",
    "    time.sleep(0.05)\n",
    "\n",
    "# Turn the big list of records into a data frame\n",
    "new_per_wp = pd.DataFrame(results)\n",
    "\n",
    "# Sum across projects to get new Wikipedia articles per month\n",
    "new_wp = new_per_wp.groupby(\"month\").agg(\n",
    "    {\"new_pages\": \"sum\"}\n",
    ").rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content metrics via Wikistats 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO: remove this code and remove metrics from the metrics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki</th>\n",
       "      <th>group</th>\n",
       "      <th>month</th>\n",
       "      <th>articles</th>\n",
       "      <th>files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>wikidata</td>\n",
       "      <td>wx</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>48917104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>wikidata</td>\n",
       "      <td>wx</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>50391825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>wikidata</td>\n",
       "      <td>wx</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>50978404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>wikidata</td>\n",
       "      <td>wx</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>51446133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>wikidata</td>\n",
       "      <td>wx</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>51977456</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wiki group      month  articles  files\n",
       "1926  wikidata    wx 2018-04-30  48917104    0.0\n",
       "1927  wikidata    wx 2018-05-31  50391825    0.0\n",
       "1928  wikidata    wx 2018-06-30  50978404    0.0\n",
       "1929  wikidata    wx 2018-07-31  51446133    0.0\n",
       "1930  wikidata    wx 2018-08-31  51977456    0.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull Wikistats data files, removing old copies from previous runs\n",
    "wikistats_files = \"csv_*_main.zip\"\n",
    "wikistats_path = \"/mnt/data/xmldatadumps/public/other/wikistats_1/\"\n",
    "!rm data/{wikistats_files}\n",
    "!cp {wikistats_path}{wikistats_files} data\n",
    "\n",
    "zipfiles = !ls data/csv_*_main.zip\n",
    "cols = [\"wiki\", \"group\", \"month\", \"articles\", \"files\"]\n",
    "content = pd.DataFrame(columns=cols)\n",
    "\n",
    "for f in zipfiles:\n",
    "    # Extract the Wikistats code for the project family\n",
    "    grp = re.search(r\"data/csv_([a-z]{2})_main.zip\", f).group(1)\n",
    "    \n",
    "    # Map Wikistats codes for project family to the corresponding database codes\n",
    "    db_suffix = {\n",
    "        \"wb\": \"wikibooks\",\n",
    "        \"wk\": \"wiktionary\",\n",
    "        \"wn\": \"wikinews\",\n",
    "        \"wo\": \"wikivoyage\",\n",
    "        \"wp\": \"wiki\",\n",
    "        \"wq\": \"wikiquote\",\n",
    "        \"ws\": \"wikisource\",\n",
    "        \"wv\": \"wikiversity\",\n",
    "        \"wx\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Unzip files to stdout and capture it in an IPython SList.\n",
    "    # Put the newline-separated string (`.n`) of the output in a buffer for Pandas.\n",
    "    sm = !unzip -p {f} StatisticsMonthly.csv\n",
    "    sm = io.StringIO(sm.n)\n",
    "    \n",
    "    spn = !unzip -p {f} StatisticsPerNamespace.csv\n",
    "    spn = io.StringIO(spn.n)\n",
    "     \n",
    "    # Manually set column numbers because some CSVs are ragged \n",
    "    # Select the columns we need, which aren't named so we need to select by location\n",
    "    art = pd.read_csv(sm, header=None, usecols=[0, 1, 6], names=range(29))\n",
    "    art.columns = [\"wiki\", \"month\", \"articles\"]\n",
    "\n",
    "    # Wikisource has extra namespaces so its file has more columns\n",
    "    if grp == \"ws\":\n",
    "        col_nums = range(22)\n",
    "    else:\n",
    "        col_nums = range(17)\n",
    "    \n",
    "    files = pd.read_csv(spn, header=None, usecols=[0, 1, 5], names=col_nums)\n",
    "    files.columns = [\"wiki\", \"month\", \"files\"]\n",
    "        \n",
    "    grp_content = pd.merge(art, files, on=[\"wiki\", \"month\"], validate=\"one_to_one\")\n",
    "        \n",
    "    # Wiki column just contains the language code (except in wx) so we have to disambiguate across files\n",
    "    grp_content[\"wiki\"] = grp_content[\"wiki\"] + db_suffix[grp]\n",
    "    \n",
    "    grp_content[\"group\"] = grp\n",
    "    \n",
    "    content = content.append(grp_content, sort=False)\n",
    "    \n",
    "    sm.close()\n",
    "    spn.close()\n",
    "\n",
    "content[\"month\"] = pd.to_datetime(content[\"month\"])\n",
    "content[\"articles\"] = content[\"articles\"].astype(int)\n",
    "\n",
    "# Remove \"wikis\" with zz codes since those are aggregates\n",
    "not_zz = lambda df: ~df[\"wiki\"].str.match(r\"zz.*\")\n",
    "content = content[not_zz]\n",
    "\n",
    "content.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_content</th>\n",
       "      <th>wikipedia_articles</th>\n",
       "      <th>files</th>\n",
       "      <th>wikidata_entities</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>188058075.0</td>\n",
       "      <td>47821184.0</td>\n",
       "      <td>50216472.0</td>\n",
       "      <td>48917104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>191224933.0</td>\n",
       "      <td>47994707.0</td>\n",
       "      <td>50950647.0</td>\n",
       "      <td>50391825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>192895236.0</td>\n",
       "      <td>48161127.0</td>\n",
       "      <td>51555579.0</td>\n",
       "      <td>50978404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>194750830.0</td>\n",
       "      <td>48378242.0</td>\n",
       "      <td>52227012.0</td>\n",
       "      <td>51446133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>196624959.0</td>\n",
       "      <td>48591849.0</td>\n",
       "      <td>53044373.0</td>\n",
       "      <td>51977456.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_content  wikipedia_articles       files  wikidata_entities\n",
       "month                                                                       \n",
       "2018-04-30    188058075.0          47821184.0  50216472.0         48917104.0\n",
       "2018-05-31    191224933.0          47994707.0  50950647.0         50391825.0\n",
       "2018-06-30    192895236.0          48161127.0  51555579.0         50978404.0\n",
       "2018-07-31    194750830.0          48378242.0  52227012.0         51446133.0\n",
       "2018-08-31    196624959.0          48591849.0  53044373.0         51977456.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Content is articles + files on all wikis except Commons, \n",
    "# where it's files alone since there files count as articles\n",
    "def count_content(df):\n",
    "    files = df[\"files\"].sum()\n",
    "    noncommons_articles = df[df[\"wiki\"] != \"commons\"][\"articles\"].sum()\n",
    "    total_content = noncommons_articles + files\n",
    "    wikipedia_articles = df[df[\"group\"] == \"wp\"][\"articles\"].sum()\n",
    "    wikidata_entities = df[df[\"wiki\"] == \"wikidata\"][\"articles\"].sum()\n",
    "\n",
    "    return pd.Series(\n",
    "        [total_content, wikipedia_articles, files, wikidata_entities],\n",
    "        index=[\"total_content\", \"wikipedia_articles\", \"files\", \"wikidata_entities\"]\n",
    "    )\n",
    "\n",
    "glob_cont = content.groupby(\"month\").apply(count_content)\n",
    "glob_cont.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>total_content</th>\n",
       "      <th>wikipedia_articles</th>\n",
       "      <th>files</th>\n",
       "      <th>wikidata_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [month, total_content, wikipedia_articles, files, wikidata_entities]\n",
       "Index: []"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dates with 0 articles, because those are junk data\n",
    "glob_cont = glob_cont[glob_cont[\"wikipedia_articles\"] != 0]\n",
    "\n",
    "# This data is calculated as of the end of a calendar month. In other places,\n",
    "# the metric is dated the first day of that month it applies to. Let's convert\n",
    "# to that.\n",
    "glob_cont.index = glob_cont.index - pd.tseries.offsets.MonthBegin()\n",
    "\n",
    "glob_cont = glob_cont[START:]\n",
    "\n",
    "# Reset index so we can merge\n",
    "glob_cont = glob_cont.reset_index()\n",
    "\n",
    "glob_cont.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and saving metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_editors</th>\n",
       "      <th>data_edits</th>\n",
       "      <th>existing_active_editors</th>\n",
       "      <th>files</th>\n",
       "      <th>mobile_edits</th>\n",
       "      <th>net_new_Commons_content_pages</th>\n",
       "      <th>net_new_Wikidata_entities</th>\n",
       "      <th>net_new_Wikipedia_articles</th>\n",
       "      <th>net_new_content_pages</th>\n",
       "      <th>new_active_editors</th>\n",
       "      <th>new_editor_retention</th>\n",
       "      <th>nonbot_nondata_nonupload_edits</th>\n",
       "      <th>revert_rate</th>\n",
       "      <th>second_month_active_editors</th>\n",
       "      <th>total_content</th>\n",
       "      <th>total_edits</th>\n",
       "      <th>uploads</th>\n",
       "      <th>wikidata_entities</th>\n",
       "      <th>wikipedia_articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>85424.0</td>\n",
       "      <td>15706189.0</td>\n",
       "      <td>62698.0</td>\n",
       "      <td>50974289.0</td>\n",
       "      <td>1165558.0</td>\n",
       "      <td>687827</td>\n",
       "      <td>1466249</td>\n",
       "      <td>177423</td>\n",
       "      <td>3119985</td>\n",
       "      <td>17787.0</td>\n",
       "      <td>0.047123</td>\n",
       "      <td>14355475.0</td>\n",
       "      <td>0.076782</td>\n",
       "      <td>4068.0</td>\n",
       "      <td>191319173.0</td>\n",
       "      <td>39081127.0</td>\n",
       "      <td>735020.0</td>\n",
       "      <td>50429005.0</td>\n",
       "      <td>48018756.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>78549.0</td>\n",
       "      <td>16473924.0</td>\n",
       "      <td>59034.0</td>\n",
       "      <td>51555579.0</td>\n",
       "      <td>1128001.0</td>\n",
       "      <td>570494</td>\n",
       "      <td>578664</td>\n",
       "      <td>177167</td>\n",
       "      <td>1640181</td>\n",
       "      <td>15005.0</td>\n",
       "      <td>0.051513</td>\n",
       "      <td>12995238.0</td>\n",
       "      <td>0.076353</td>\n",
       "      <td>3664.0</td>\n",
       "      <td>192895236.0</td>\n",
       "      <td>37115031.0</td>\n",
       "      <td>604437.0</td>\n",
       "      <td>50978404.0</td>\n",
       "      <td>48161127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>77734.0</td>\n",
       "      <td>14115389.0</td>\n",
       "      <td>59386.0</td>\n",
       "      <td>52227012.0</td>\n",
       "      <td>1169233.0</td>\n",
       "      <td>632174</td>\n",
       "      <td>459588</td>\n",
       "      <td>221481</td>\n",
       "      <td>1811794</td>\n",
       "      <td>14037.0</td>\n",
       "      <td>0.056814</td>\n",
       "      <td>13385582.0</td>\n",
       "      <td>0.076803</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>194750830.0</td>\n",
       "      <td>36942656.0</td>\n",
       "      <td>665106.0</td>\n",
       "      <td>51446133.0</td>\n",
       "      <td>48378242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>78681.0</td>\n",
       "      <td>18261169.0</td>\n",
       "      <td>60371.0</td>\n",
       "      <td>53044373.0</td>\n",
       "      <td>1226835.0</td>\n",
       "      <td>765093</td>\n",
       "      <td>521824</td>\n",
       "      <td>221601</td>\n",
       "      <td>1822092</td>\n",
       "      <td>14018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13890731.0</td>\n",
       "      <td>0.066912</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>196624959.0</td>\n",
       "      <td>40968361.0</td>\n",
       "      <td>802116.0</td>\n",
       "      <td>51977456.0</td>\n",
       "      <td>48591849.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>82084.0</td>\n",
       "      <td>17797137.0</td>\n",
       "      <td>59265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1171268.0</td>\n",
       "      <td>804334</td>\n",
       "      <td>377916</td>\n",
       "      <td>187533</td>\n",
       "      <td>1641871</td>\n",
       "      <td>18284.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13445328.0</td>\n",
       "      <td>0.068043</td>\n",
       "      <td>3671.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38570897.0</td>\n",
       "      <td>840120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            active_editors  data_edits  existing_active_editors       files  \\\n",
       "month                                                                         \n",
       "2018-05-01         85424.0  15706189.0                  62698.0  50974289.0   \n",
       "2018-06-01         78549.0  16473924.0                  59034.0  51555579.0   \n",
       "2018-07-01         77734.0  14115389.0                  59386.0  52227012.0   \n",
       "2018-08-01         78681.0  18261169.0                  60371.0  53044373.0   \n",
       "2018-09-01         82084.0  17797137.0                  59265.0         NaN   \n",
       "\n",
       "            mobile_edits  net_new_Commons_content_pages  \\\n",
       "month                                                     \n",
       "2018-05-01     1165558.0                         687827   \n",
       "2018-06-01     1128001.0                         570494   \n",
       "2018-07-01     1169233.0                         632174   \n",
       "2018-08-01     1226835.0                         765093   \n",
       "2018-09-01     1171268.0                         804334   \n",
       "\n",
       "            net_new_Wikidata_entities  net_new_Wikipedia_articles  \\\n",
       "month                                                               \n",
       "2018-05-01                    1466249                      177423   \n",
       "2018-06-01                     578664                      177167   \n",
       "2018-07-01                     459588                      221481   \n",
       "2018-08-01                     521824                      221601   \n",
       "2018-09-01                     377916                      187533   \n",
       "\n",
       "            net_new_content_pages  new_active_editors  new_editor_retention  \\\n",
       "month                                                                         \n",
       "2018-05-01                3119985             17787.0              0.047123   \n",
       "2018-06-01                1640181             15005.0              0.051513   \n",
       "2018-07-01                1811794             14037.0              0.056814   \n",
       "2018-08-01                1822092             14018.0                   NaN   \n",
       "2018-09-01                1641871             18284.0                   NaN   \n",
       "\n",
       "            nonbot_nondata_nonupload_edits  revert_rate  \\\n",
       "month                                                     \n",
       "2018-05-01                      14355475.0     0.076782   \n",
       "2018-06-01                      12995238.0     0.076353   \n",
       "2018-07-01                      13385582.0     0.076803   \n",
       "2018-08-01                      13890731.0     0.066912   \n",
       "2018-09-01                      13445328.0     0.068043   \n",
       "\n",
       "            second_month_active_editors  total_content  total_edits   uploads  \\\n",
       "month                                                                           \n",
       "2018-05-01                       4068.0    191319173.0   39081127.0  735020.0   \n",
       "2018-06-01                       3664.0    192895236.0   37115031.0  604437.0   \n",
       "2018-07-01                       3455.0    194750830.0   36942656.0  665106.0   \n",
       "2018-08-01                       3433.0    196624959.0   40968361.0  802116.0   \n",
       "2018-09-01                       3671.0            NaN   38570897.0  840120.0   \n",
       "\n",
       "            wikidata_entities  wikipedia_articles  \n",
       "month                                              \n",
       "2018-05-01         50429005.0          48018756.0  \n",
       "2018-06-01         50978404.0          48161127.0  \n",
       "2018-07-01         51446133.0          48378242.0  \n",
       "2018-08-01         51977456.0          48591849.0  \n",
       "2018-09-01                NaN                 NaN  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MariaDB results\n",
    "dfs = [mdb_queries[k][\"result\"] for k in mdb_queries]\n",
    "\n",
    "# Hive results\n",
    "dfs.extend([hive_queries[k][\"result\"] for k in hive_queries])\n",
    "\n",
    "# AQS content results\n",
    "dfs.extend([total_new, new_commons, new_wd, new_wp])\n",
    "\n",
    "# Wikistats 1 content results\n",
    "dfs.append(glob_cont)\n",
    "\n",
    "# Merge them all, assuming that the month is the only common column\n",
    "new_metrics = reduce(lambda l, r: pd.merge(l, r, how=\"outer\"), dfs)\n",
    "\n",
    "# Set the month as an index so combine_first works properly\n",
    "new_metrics = new_metrics.set_index(\"month\")\n",
    "\n",
    "if old_metrics is not None:\n",
    "    metrics = new_metrics.combine_first(old_metrics)\n",
    "else:\n",
    "    metrics = new_metrics\n",
    "    \n",
    "metrics = metrics.sort_index()\n",
    "\n",
    "metrics.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv(FILENAME, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
